from __gin__ import dynamic_registration

import __main__ as train_script

from t5.data import mixtures
from t5x import models
from t5x import partitioning
from t5x import utils
import customize_tasks
import tasks

include 't5x/configs/runs/finetune.gin'
include 't5x/examples/t5/mt5/xxl.gin'

BATCH_SIZE = 256
#MIXTURE_OR_TASK_NAME = "clueai_mt"
MIXTURE_OR_TASK_NAME = "clueai_corpus_mt"
TASK_FEATURE_LENGTHS = {'inputs': 256, 'targets': 256}
DROPOUT_RATE = 0.05
TRAIN_STEPS = 2_200_000  # 1000000 pre-trained steps + 20000 fine-tuning steps.
USE_CACHED_TASKS = False

#INITIAL_CHECKPOINT_PATH = "gs://t5-data/pretrained_models/t5x/mt5_lm_adapted/xxl/checkpoint_1100000"
INITIAL_CHECKPOINT_PATH = "gs://clueai/models/zxw/mt5_t5x_xxl/checkpoint_1200000"

# `LOSS_NORMALIZING_FACTOR`: When fine-tuning a model that was pre-trained
# using Mesh Tensorflow (e.g. the public T5 / mT5 / ByT5 models), this should be
# set to `pretraining batch_size` * `target_token_length`. For T5 and T5.1.1:
# `2048 * 114`. For mT5: `1024 * 229`. For ByT5: `1024 * 189`.
LOSS_NORMALIZING_FACTOR = 234496

train_script.train:
  eval_period = 8000
  eval_steps = 1
  random_seed = 0
  use_hardware_rng = True

from t5x import partitioning

train_script.train:
  partitioner = @partitioning.PjitPartitioner()

#partitioning.PjitPartitioner:
#  num_partitions = 1
#  logical_axis_rules= @partitioning.standard_logical_axis_rules()
#
#partitioning.standard_logical_axis_rules:
#  activation_partitioning_dims = 2
#  parameter_partitioning_dims = 2

partitioning.PjitPartitioner.num_partitions = 8

partitioning.PjitPartitioner.logical_axis_rules = [
    ('batch', 'data'),
    ('mlp', 'model'),
    ('heads', 'model'),
    ('vocab', 'model'),
    # shard both activations and weight matrices on the remaining available axis
    ('embed', 'model'),
    ('embed', 'data'),
    ('kv', None),
    ('joined_kv', None),
    ('relpos_buckets', None),
    ('abspos_buckets', None),
    ('length', None),
    ('layers', None),
    ('stack', None),
    ('mlp_activations', None),
]


train/utils.DatasetConfig:
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'train'
  batch_size = %BATCH_SIZE
  shuffle = True
  seed = None  # use a new seed each run/restart
  use_cached = %USE_CACHED_TASKS
  pack = True
  module = %MIXTURE_OR_TASK_MODULE

train_eval/utils.DatasetConfig:
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'dev'
  batch_size = %BATCH_SIZE
  shuffle = False
  seed = 42
  use_cached = %USE_CACHED_TASKS
  pack = True
  module = %MIXTURE_OR_TASK_MODULE

infer_eval/utils.DatasetConfig:
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = None  # compute max
  split = 'dev'
  batch_size = %BATCH_SIZE
  shuffle = False
  seed = 42
  use_cached = %USE_CACHED_TASKS
  pack = False
  module = %MIXTURE_OR_TASK_MODULE

# bfloat16
utils.RestoreCheckpointConfig:
  path = %INITIAL_CHECKPOINT_PATH
  mode = 'specific'
  dtype = 'float32'

utils.SaveCheckpointConfig:
  period = 8000  # checkpoint frequency
  dtype = 'float32'
  #keep = 1
  keep = None 

utils.create_learning_rate_scheduler:
  factors = 'constant'
  base_learning_rate = 0.0005
  warmup_steps = 1000